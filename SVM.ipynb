{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-26T18:01:24.986602Z",
     "iopub.status.busy": "2024-02-26T18:01:24.986218Z",
     "iopub.status.idle": "2024-02-26T18:01:25.485350Z",
     "shell.execute_reply": "2024-02-26T18:01:25.484152Z",
     "shell.execute_reply.started": "2024-02-26T18:01:24.986575Z"
    },
    "id": "EmBxlB7VYAF2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This reads the already generated pre-processing file in the Voting Notebook\n",
    "df = pd.read_csv('elmo_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INHIBITOR                 2454\n",
       "SUBSTRATE                  802\n",
       "INDIRECT-DOWNREGULATOR     757\n",
       "INDIRECT-UPREGULATOR       665\n",
       "ACTIVATOR                  580\n",
       "ANTAGONIST                 434\n",
       "PRODUCT-OF                 363\n",
       "AGONIST                    267\n",
       "DOWNREGULATOR              152\n",
       "UPREGULATOR                 84\n",
       "SUBSTRATE_PRODUCT-OF        19\n",
       "AGONIST-ACTIVATOR           15\n",
       "AGONIST-INHIBITOR            4\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just a check for EDA to see counts of relations\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the preproessed test file already generated in the voting Notebook\n",
    "test=pd.read_csv('test_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7090909090909091\n",
      "Accuracy: 0.5332948976650331\n"
     ]
    }
   ],
   "source": [
    "# Function to convert ELMo embeddings string to numeric format\n",
    "def convert_elmo_string_to_array(elmo_str):\n",
    "    return np.array([float(x) for x in elmo_str.split(',')])\n",
    "\n",
    "# Function to calculate the novelty feature of Relative \n",
    "def calculate_relative_positions(sentence, entity1, entity2):\n",
    "    # Ensure entity1 and entity2 are treated as strings\n",
    "    entity1 = str(entity1).strip()\n",
    "    entity2 = str(entity2).strip()\n",
    "    \n",
    "    words = sentence.split()\n",
    "    len_sentence = len(words)\n",
    "    # Using str.find() safely by ensuring entity1 and entity2 are strings\n",
    "    entity1_pos = sentence.find(entity1) / len(sentence) if entity1 in sentence else -1\n",
    "    entity2_pos = sentence.find(entity2) / len(sentence) if entity2 in sentence else -1\n",
    "    relative_positions = [abs(i / len_sentence - entity1_pos) + abs(i / len_sentence - entity2_pos) for i, _ in enumerate(words)]\n",
    "    \n",
    "    # Handling cases where entities might not be found in the sentence\n",
    "    if entity1_pos == -1 or entity2_pos == -1:\n",
    "        return 0  # Or some default value, adjust based on your requirements\n",
    "    \n",
    "    return np.mean(relative_positions)  # Using the mean relative position as a feature\n",
    "\n",
    "# Convert ELMo embeddings strings to arrays\n",
    "df['elmo_embeddings'] = df['elmo_embeddings_str'].apply(convert_elmo_string_to_array)\n",
    "test['elmo_embeddings'] = test['elmo_embeddings_str'].apply(convert_elmo_string_to_array)\n",
    "# Calculate relative position features\n",
    "df['relative_position_features'] = df.apply(lambda row: calculate_relative_positions(row['text'], row['E1'], row['E2']), axis=1)\n",
    "test['relative_position_features'] = test.apply(lambda row: calculate_relative_positions(row['text'], row['E1'], row['E2']), axis=1)\n",
    "# Prepare dataset for SVM\n",
    "X = np.array([np.append(embeddings, pos_feature) for embeddings, pos_feature in zip(df['elmo_embeddings'], df['relative_position_features'])])\n",
    "test_val= np.array([np.append(embeddings, pos_feature) for embeddings, pos_feature in zip(test['elmo_embeddings'], test['relative_position_features'])])\n",
    "y = df['label'].values  # Assuming you have a 'label' column for classification\n",
    "test_la=test['label'].values\n",
    "# Example: Splitting data and training an SVM\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "test_scaled=scaler.transform(test_val)\n",
    "# Train SVM\n",
    "svm_model = SVC(kernel='rbf')\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions = svm_model.predict(X_test_scaled)\n",
    "predictions_test= svm_model.predict(test_scaled)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracy_2 = accuracy_score(test_la, predictions_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Accuracy:\", accuracy_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "             ACTIVATOR       0.65      0.45      0.53       112\n",
      "               AGONIST       0.57      0.40      0.47        42\n",
      "     AGONIST-ACTIVATOR       0.00      0.00      0.00         3\n",
      "     AGONIST-INHIBITOR       0.00      0.00      0.00         2\n",
      "            ANTAGONIST       0.88      0.79      0.83        99\n",
      "         DOWNREGULATOR       0.77      0.52      0.62        33\n",
      "INDIRECT-DOWNREGULATOR       0.66      0.61      0.63       151\n",
      "  INDIRECT-UPREGULATOR       0.58      0.66      0.62       129\n",
      "             INHIBITOR       0.72      0.92      0.80       466\n",
      "            PRODUCT-OF       0.66      0.58      0.62        77\n",
      "             SUBSTRATE       0.84      0.68      0.75       181\n",
      "  SUBSTRATE_PRODUCT-OF       0.00      0.00      0.00         7\n",
      "           UPREGULATOR       0.67      0.11      0.19        18\n",
      "\n",
      "              accuracy                           0.71      1320\n",
      "             macro avg       0.54      0.44      0.47      1320\n",
      "          weighted avg       0.70      0.71      0.70      1320\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jithi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jithi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jithi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jithi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jithi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jithi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming 'predictions' and 'y_test' are already defined from your model's output\n",
    "report = classification_report(y_test, predictions, output_dict=True)\n",
    "\n",
    "# Printing the classification report\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# Accessing F1 score, precision, and recall for each class\n",
    "for label, metrics in report.items():\n",
    "    if label.isdigit():  # Check to ensure processing class labels, adjust as necessary\n",
    "        print(f\"Class: {label}, Precision: {metrics['precision']}, Recall: {metrics['recall']}, F1-score: {metrics['f1-score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "             ACTIVATOR       0.32      0.20      0.24       292\n",
      "               AGONIST       0.57      0.21      0.31       182\n",
      "     AGONIST-ACTIVATOR       0.00      0.00      0.00         4\n",
      "     AGONIST-INHIBITOR       0.00      0.00      0.00        12\n",
      "            ANTAGONIST       0.74      0.56      0.64       293\n",
      "         DOWNREGULATOR       0.00      0.00      0.00        72\n",
      "INDIRECT-DOWNREGULATOR       0.41      0.34      0.37       340\n",
      "  INDIRECT-UPREGULATOR       0.40      0.33      0.36       334\n",
      "             INHIBITOR       0.55      0.91      0.69      1255\n",
      "            PRODUCT-OF       0.60      0.18      0.28       191\n",
      "             SUBSTRATE       0.59      0.42      0.49       453\n",
      "           UPREGULATOR       0.00      0.00      0.00        41\n",
      "\n",
      "              accuracy                           0.53      3469\n",
      "             macro avg       0.35      0.26      0.28      3469\n",
      "          weighted avg       0.51      0.53      0.49      3469\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jithi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jithi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jithi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jithi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jithi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jithi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming 'predictions' and 'y_test' are already defined from your model's output\n",
    "report = classification_report(test_la, predictions_test, output_dict=True)\n",
    "\n",
    "# Printing the classification report\n",
    "print(classification_report(test_la, predictions_test))\n",
    "\n",
    "# Accessing F1 score, precision, and recall for each class\n",
    "for label, metrics in report.items():\n",
    "    if label.isdigit():  # Check to ensure processing class labels, adjust as necessary\n",
    "        print(f\"Class: {label}, Precision: {metrics['precision']}, Recall: {metrics['recall']}, F1-score: {metrics['f1-score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4499337,
     "sourceId": 7706509,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
